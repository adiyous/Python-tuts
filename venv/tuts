import datetime
from datetime import time, date, timedelta
import time

import os
from os import path

import shutil
from shutil import make_archive
from zipfile import ZipFile

import urllib.request

import json

# def main():
    # today = date.today()
    # print("Today is ", today)
    # print("Dtae: ", today.day, today.month, today.year)
    # print("Weekday is: ", today.weekday())

    # today = datetime.now()
    # print(today)
    # now = datetime.now()
    # print(now.strftime("The current year is: %a %d %B %Y"))

    # f = open('textfile.txt', 'w+')
    # for i in range(10):
    #     f.write('This is a line ' + str(i) + '\r\n')
    #
    # f.close()

    # f = open('textfile.txt', 'r')
    # if f.mode == 'r':
    #     print(f.read())

    # print(os.name)
    # print("Item exists: " + str(path.exists('textfile.txt')))
    # print("Item is a file: " + str(path.isfile('textfile.txt')))
    # print("Item is a directory : " + str(path.isdir('textfile.txt')))
    # print("Item path : " + str(path.realpath('textfile.txt')))
    # t = time.ctime(path.getmtime('textfile.txt'))
    # print(t)
    # if path.exists('textfile.txt'):
    #     src = path.realpath('textfile.txt')
    #     dst = src + '.bak'
    #     # shutil.copystat(src, dst)
    #     # os.rename('textfile.txt.bak', 'newtextFile.txt')
    #     # root_dir, tail = path.split(src)
    #     # shutil.make_archive('archive', 'zip', root_dir)
    #     with ZipFile('testzip.zip', 'w') as newzip:
    #         newzip.write('textfile.txt')
    # webUrl = urllib.request.urlopen("http://www.google.com")
    # print('result code: ' + str(webUrl.getcode()))
    # data = webUrl.read()
    # print(data)
# def printResults(data):
#      theJSON = json.loads(data)
#      if "title" in theJSON['metadata']:
#          print(theJSON['metadata']['title'])
#
#      count = theJSON['metadata']['count']
#      print(str(count) + " events")
#
#      for i in theJSON['features']:
#          print(i['properties']['place'])
#      print("---------------------------\n")
#
#      for i in theJSON['features']:
#          if i['properties']['mag'] >= 4:
#             print(i['properties']['mag'], i['properties']['place'])
#      print("---------------------------\n")
#
#      for i in theJSON['features']:
#          felt = i['properties']['felt']
#          if felt != None:
#              if felt > 0:
#                  print('%2.1f' %
#                        i['properties']['mag'],
#                        i['properties']['place'],
#                        " reported " +
#                        str(felt) + " times"
#                        )
#
# def main():
#     urlData = "http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_day.geojson"
#     webUrl = urllib.request.urlopen(urlData)
#     print("result code: " + str(webUrl.getcode()))
#     if (webUrl.getcode() == 200):
#         data = webUrl.read()
#         printResults(data)
#     else:
#         print("Cannot parse results")
# from html.parser import HTMLParser

# metacount = 0

# class MyHTMLParser(HTMLParser):
#     def handle_comment(self, data):
#         print('Encountered comment: ', data)
#         pos = self.getpos()
#         print('\tAt line: ', pos[0], ' position ', pos[1])

#     def handle_startendtag(self, tag, attrs):
#         global metacount
#         if tag == 'meta':
#             metacount += 1
#         print('Encountered tag: ', tag)
#         pos = self.getpos()
#         print('\tAt line: ', pos[0], ' position ', pos[1])
#         if attrs.__len__() > 0:
#             print('\tAttributes:')
#             for a in attrs:
#                 print('\t', a[0], '=', a[1])

#     def handle_endtag(self, tag):
#         print('Encountered tag: ', tag)
#         pos = self.getpos()
#         print('\tAt line: ', pos[0], ' position ', pos[1])

#     def handle_data(self, data):
#         if data.isspace():
#             return
#         print('Encountered data: ', data)
#         pos = self.getpos()
#         print('\tAt line: ', pos[0], ' position ', pos[1])

# def main():
#     parser = MyHTMLParser()
#     f = open('sample.html')
#     if f.mode == 'r':
#         contents = f.read()
#         parser.feed(contents)

#     print("Meta tags found: " + str(metacount))
import xml.dom.minidom

def main():
    doc = xml.dom.minidom.parse('sample.xml')
    print(doc.nodeName)
    print(doc.firstChild.tagName)

    skills = doc.getElementsByTagName("skill")
    print("%d skills: " % skills.length)

    for skill in skills:
        print(skill.getAttribute("name"))
        
        newSkill = doc.createElement("skill")
        newSkill.setAttribute("name", "jQuery")
        doc.firstChild.appendChild(newSkill)

    print("%d skills: " % skills.length)
    for skill in skills:
        print(skill.getAttribute("name"))

if __name__ == "__main__":
    main()
